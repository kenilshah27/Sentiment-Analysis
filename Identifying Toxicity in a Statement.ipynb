{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/glovembedding/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport time\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Input\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,CuDNNLSTM, GlobalAveragePooling1D,Concatenate\nfrom keras.optimizers import Adam,RMSprop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data[['target','comment_text']]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'] = train['target'].apply(lambda x: 1 if x > 0.5 else 0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['target']\ny_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    text = text.str.lower()\n    text = text.str.replace(r'\\r',' ')\n    text = text.str.replace(r'\\n',' ')\n    text = text.str.replace('[^a-zA-Z0-9]',' ')\n    text = text.apply(lambda x: \" \".join(x.split()))\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['comment_text']\nX = clean(X)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Variable Declaration\n\nMAX_SEQUENCE_LENGTH = 200\nMAX_VOCAB_SIZE = 50000\nEMBEDDING_DIM = 50\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nLSTM_UNITS_2 = 128\nEPOCHS = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizer\nstime = time.time()\n\nsentences = X.values\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\n\netime = time.time()\n# Words are converted into integers for the model\n\nprint(sentences[0])\nprint(sequences[0])\n\nt = etime-stime\n\nprint('Execution Time: ',t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(len(s.split()) for s in sentences) # As we see that the maximum length is 327 but we are limiting it to 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datafinal = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', datafinal.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the word to index mapping\nword2idx = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nnum_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word2idx) # There are 305408 unique words from the dataset but we are limiting the number of words 20000.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) # Creating the embedding matrix for 50000 words , each having a EMBEDDING_DIM of 50\nembedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the Word2Vec vector from predefined glove word vectors\n\nstime = time.time()\n\nword2vec = {}\nwith open(os.path.join('../input/glovembedding/glove.6B.50d.txt')) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\n\netime = time.time()\nt = etime-stime\nprint('Found %s word vectors.' % len(word2vec))\nprint('Execution time: ',t)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the embedding vector for all our words \n\nstime = time.time()\n\nfor word, i in word2idx.items():\n    if i < MAX_VOCAB_SIZE:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all zeros.\n            embedding_matrix[i] = embedding_vector\n\netime = time.time()\nt = etime-stime\nprint('Execution time: ',t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding an embedding layer to the model\n\nembedding_layer = Embedding(\n  num_words,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False                   # as the embeddings are pretrained\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of the input: ',datafinal.shape)  # Batch size = 1804874 and Sequence Length = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))  ## First Layer of the network\nprint('Shape :',input_.shape)     ## It expects every row to have 200 columns or 200 words in our case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Adding the embedding layer to our input\n\nx = embedding_layer(input_)                 ## Second Layer of the network\nprint('Shape :',x.shape)   ## Now we see that the size has become 200*50 as each of the word is now represented by 50 vectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Adding a Bidirectional LSTM layer to the embedding output\n\nx = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x) ## Third Layer of the network\nx = Dropout(0.2)(x)\nprint('Shape :',x.shape)\nx = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x) ## Third Layer of the network\nx = Dropout(0.2)(x)\nprint('Shape :',x.shape)                     ## Now we see a size of 200 as the the 50 size vector is now represented by 200 hidden states of the LSTM considering Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Adding a MaxPooling Layer\n\nx1 = GlobalMaxPool1D()(x)         ## Fourth Layer of the network\nx2 = GlobalAveragePooling1D()(x)\nconcatenate = Concatenate()\nx = concatenate([x1,x2])\nprint('Shape :',x.shape)         ## It has performed a maximum function on axis 1 and now we have 2 dim instead of 3 as required by the Dense Layer to follow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Adding Dense Layers\n\n# x = Dense(1024,activation = 'relu')(x)  # Fifth Layer\n# x = Dropout(0.2)(x)\n# print('Shape :',x.shape)            # Now the size has changed form 200 to 512\nx = Dense(512,activation = 'relu')(x)  # Sixth Layer\nx = Dropout(0.2)(x)\nprint('Shape :',x.shape)           \nx = Dense(64,activation = 'relu')(x)   # Seventh Layer\nx = Dropout(0.2)(x)\nprint('Shape :',x.shape)            \noutput = Dense(1, activation=\"sigmoid\")(x)  # Output Layer\nprint('Shape :',output.shape)            # Final output\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(inputs = input_, outputs = output)  # Initialting the model\n\nmodel.compile(\n  loss='binary_crossentropy',     ## Assiging Loss \n  optimizer=Adam(lr=0.01,amsgrad = True),        ## Optimizer with Learning Rate\n  metrics=['accuracy']            ## Metric \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = model.fit(\n  datafinal,\n  y_train,\n  batch_size=BATCH_SIZE,\n  epochs=EPOCHS,\n  validation_split = 0.1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = clean(test['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = tokenizer.texts_to_sequences(test_data)\ntest_data = pad_sequences(test_data, maxlen=MAX_SEQUENCE_LENGTH)\ntest_data[0:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stime = time.time()\n\ny = model.predict(test_data)\n\netime = time.time()\n\nprint(etime-stime)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y = np.where(y>=0.5,1,0)\n#y = y.astype(np.int32)\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\nsub_df['prediction'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}